/home/rgundaka/miniconda3/envs/rishig/lib/python3.6/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
wandb: Currently logged in as: rishigundakaram (use `wandb login --relogin` to force relogin)
loading data
loaded data
wandb: wandb version 0.12.18 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run dashing-haze-182
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rishigundakaram/CPINO
wandb: üöÄ View run at https://wandb.ai/rishigundakaram/CPINO/runs/2uvuugn9
wandb: Run data is saved locally in /central/groups/tensorlab/rgundaka/code/PINO/CPINO/experiments/Darcy_cpino/wandb/run-20220614_150105-2uvuugn9
wandb: Run `wandb offline` to turn off syncing.

  0% 0/50 [00:00<?, ?it/s]Epoch: 0, train loss: 4.84182, f_loss_w: 0.19744, f_loss_uw: 4.84182, data loss: 0.46287:   0% 0/50 [05:42<?, ?it/s]Epoch: 0, train loss: 4.84182, f_loss_w: 0.19744, f_loss_uw: 4.84182, data loss: 0.46287:   2% 1/50 [05:42<4:39:24, 342.14s/it]Epoch: 1, train loss: 3.65036, f_loss_w: 3.65121, f_loss_uw: 3.65036, data loss: 0.14852:   2% 1/50 [13:04<4:39:24, 342.14s/it]Epoch: 1, train loss: 3.65036, f_loss_w: 3.65121, f_loss_uw: 3.65036, data loss: 0.14852:   4% 2/50 [13:04<5:16:03, 395.08s/it]Epoch: 2, train loss: 3.63269, f_loss_w: 1.90504, f_loss_uw: 3.63269, data loss: 0.14514:   4% 2/50 [18:01<5:16:03, 395.08s/it]Epoch: 2, train loss: 3.63269, f_loss_w: 1.90504, f_loss_uw: 3.63269, data loss: 0.14514:   6% 3/50 [18:01<4:41:09, 358.92s/it]Epoch: 3, train loss: 6.37341, f_loss_w: 3.74252, f_loss_uw: 6.37341, data loss: 0.17980:   6% 3/50 [22:38<4:41:09, 358.92s/it]Epoch: 3, train loss: 6.37341, f_loss_w: 3.74252, f_loss_uw: 6.37341, data loss: 0.17980:   8% 4/50 [22:38<4:16:44, 334.88s/it]Epoch: 4, train loss: 4.38825, f_loss_w: 1.94509, f_loss_uw: 4.38825, data loss: 0.14137:   8% 4/50 [26:16<4:16:44, 334.88s/it]Epoch: 4, train loss: 4.38825, f_loss_w: 1.94509, f_loss_uw: 4.38825, data loss: 0.14137:  10% 5/50 [26:16<3:49:51, 306.48s/it]Epoch: 5, train loss: 3.20364, f_loss_w: 0.66659, f_loss_uw: 3.20364, data loss: 0.09055:  10% 5/50 [31:25<3:49:51, 306.48s/it]Epoch: 5, train loss: 3.20364, f_loss_w: 0.66659, f_loss_uw: 3.20364, data loss: 0.09055:  12% 6/50 [31:25<3:45:01, 306.86s/it]Epoch: 6, train loss: 2.59582, f_loss_w: 0.64375, f_loss_uw: 2.59582, data loss: 0.08607:  12% 6/50 [37:41<3:45:01, 306.86s/it]Epoch: 6, train loss: 2.59582, f_loss_w: 0.64375, f_loss_uw: 2.59582, data loss: 0.08607:  14% 7/50 [37:41<3:49:29, 320.21s/it]Epoch: 7, train loss: 2.24773, f_loss_w: 0.63164, f_loss_uw: 2.24773, data loss: 0.06856:  14% 7/50 [43:17<3:49:29, 320.21s/it]Epoch: 7, train loss: 2.24773, f_loss_w: 0.63164, f_loss_uw: 2.24773, data loss: 0.06856:  16% 8/50 [43:17<3:46:08, 323.06s/it]Epoch: 8, train loss: 2.08175, f_loss_w: 0.32935, f_loss_uw: 2.08175, data loss: 0.05935:  16% 8/50 [47:38<3:46:08, 323.06s/it]Epoch: 8, train loss: 2.08175, f_loss_w: 0.32935, f_loss_uw: 2.08175, data loss: 0.05935:  18% 9/50 [47:38<3:33:45, 312.81s/it]Epoch: 9, train loss: 1.93805, f_loss_w: 0.24155, f_loss_uw: 1.93805, data loss: 0.05219:  18% 9/50 [52:13<3:33:45, 312.81s/it]Epoch: 9, train loss: 1.93805, f_loss_w: 0.24155, f_loss_uw: 1.93805, data loss: 0.05219:  20% 10/50 [52:13<3:24:43, 307.08s/it]Epoch: 10, train loss: 1.81918, f_loss_w: 0.24193, f_loss_uw: 1.81918, data loss: 0.05008:  20% 10/50 [57:21<3:24:43, 307.08s/it]Epoch: 10, train loss: 1.81918, f_loss_w: 0.24193, f_loss_uw: 1.81918, data loss: 0.05008:  22% 11/50 [57:21<3:19:38, 307.13s/it]Epoch: 11, train loss: 1.71440, f_loss_w: 0.26811, f_loss_uw: 1.71440, data loss: 0.04646:  22% 11/50 [1:03:08<3:19:38, 307.13s/it]Epoch: 11, train loss: 1.71440, f_loss_w: 0.26811, f_loss_uw: 1.71440, data loss: 0.04646:  24% 12/50 