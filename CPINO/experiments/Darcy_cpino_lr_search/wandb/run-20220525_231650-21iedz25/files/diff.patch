diff --git a/CGD_PINO/configs/Darcy-CGD-pretrain.yaml b/CGD_PINO/configs/Darcy-CGD-pretrain.yaml
deleted file mode 100644
index 2b5ba70..0000000
--- a/CGD_PINO/configs/Darcy-CGD-pretrain.yaml
+++ /dev/null
@@ -1,36 +0,0 @@
-data:
-  name: 'Darcy'
-  datapath: '/groups/tensorlab/rgundaka/code/data/piececonst_r421_N1024_smooth1.mat'
-  total_num: 1024
-  offset: 0
-  n_sample: 1000
-  nx: 421
-  sub: 7
-
-model:
-  layers: [64, 64, 64, 64, 64]
-  modes1: [20, 20, 20, 20]
-  modes2: [20, 20, 20, 20]
-  fc_dim: 128
-  activation: gelu
-  competitive: True
-
-train:
-  batchsize: 20
-  epochs: 100
-  milestones: [100, 150, 200]
-  lr_max: 0.001
-  lr_min: 0.001
-  momentum: 0
-  scheduler_gamma: 0.5
-  f_loss: 1.0
-  xy_loss: 0
-  save_dir: 'darcy-FDM'
-  save_name: 'darcy-pretrain-cgd-pino.pt'
-
-others:
-  project: 'CGD-PINO'
-  group: 'Darcy'
-  entity: 'rishigundakaram'
-
-
diff --git a/CGD_PINO/configs/Darcy-pretrain.yaml b/CGD_PINO/configs/Darcy-pretrain.yaml
deleted file mode 100644
index abc6e1e..0000000
--- a/CGD_PINO/configs/Darcy-pretrain.yaml
+++ /dev/null
@@ -1,33 +0,0 @@
-data:
-  name: 'Darcy'
-  datapath: '/groups/tensorlab/rgundaka/code/data/piececonst_r421_N1024_smooth1.mat'
-  total_num: 1024
-  offset: 0
-  n_sample: 1000
-  nx: 421
-  sub: 7
-
-model:
-  layers: [64, 64, 64, 64, 64]
-  modes1: [20, 20, 20, 20]
-  modes2: [20, 20, 20, 20]
-  fc_dim: 128
-  activation: gelu
-  competitive: False
-
-train:
-  batchsize: 20
-  epochs: 300
-  milestones: [100, 150, 200]
-  base_lr: .001
-  scheduler_gamma: 0.5
-  f_loss: 1.0
-  xy_loss: 0
-  save_dir: 'darcy-FDM'
-  save_name: 'darcy-pretrain-pino.pt'
-
-others:
-  project: 'CGD-PINO'
-  group: 'Darcy'
-  entity: 'rishigundakaram'
-
diff --git a/CGD_PINO/model.py b/CGD_PINO/model.py
deleted file mode 100644
index 471834e..0000000
--- a/CGD_PINO/model.py
+++ /dev/null
@@ -1,35 +0,0 @@
-from code.PINO.train_utils.losses import weighted_darcy_loss
-import torch 
-import torch.nn as nn
-import torch.nn.functional as F
-
-from CGDs import BCGD
-
-from models.fourier2d import FNN2d
-from models.fourier3d import FNN3d
-
-from train_utils.losses import weighted_darcy_loss, darcy_loss
-
-class CGD_PINO_2D(): 
-    def __init__(self, params) -> None:
-        Weighter = FNN2d(params)
-        Regressor = FNN2d(params)
-        optimizer = BCGD(max_params=Weighter.params(), 
-                        min_params=Regressor.params(), 
-                        lr_min=params['lr_min'], 
-                        lr_max=params['lr_max'],
-                        momentum=params['momentum'])
-        w_loss = None
-        uw_loss = None
-    def __call__(self, x):
-        return self.Regressor(x)
-    
-    def loss(self, pred, a): 
-        w = self.Weighter(a)
-        w_loss, uw_loss = weighted_darcy_loss(pred, a, w)
-        self.w_loss = w_loss
-        self.uw_loss = uw_loss
-        return w_loss
-
-    def step(self): 
-        self.optimizer.step(loss=self.w_loss)
diff --git a/eval_operator.py b/eval_operator.py
index fdedbb1..483d903 100644
--- a/eval_operator.py
+++ b/eval_operator.py
@@ -45,7 +45,7 @@ def test_3d(config):
             device=device)
 
 
-def test_2d(config):
+def test_2d(config, args):
     device = 0 if torch.cuda.is_available() else 'cpu'
     data_config = config['data']
     dataset = DarcyFlow(data_config['datapath'],
@@ -64,21 +64,21 @@ def test_2d(config):
         ckpt = torch.load(ckpt_path)
         model.load_state_dict(ckpt['model'])
         print('Weights loaded from %s' % ckpt_path)
-    eval_darcy(model, dataloader, config, device)
+    eval_darcy(model, dataloader, config, device, log=args.log, entity=config['others']['entity'])
 
 
 if __name__ == '__main__':
     parser = ArgumentParser(description='Basic paser')
     parser.add_argument('--config_path', type=str, help='Path to the configuration file')
     parser.add_argument('--log', action='store_true', help='Turn on the wandb')
-    options = parser.parse_args()
-    config_file = options.config_path
+    args = parser.parse_args()
+    config_file = args.config_path
     with open(config_file, 'r') as stream:
         config = yaml.load(stream, yaml.FullLoader)
 
     if 'name' in config['data'] and config['data']['name'] == 'Darcy':
-        test_2d(config)
+        test_2d(config, args)
     else:
-        test_3d(config)
+        test_3d(config, args)
 
 
diff --git a/train_operator.py b/train_operator.py
index 58e617a..2a42aa5 100644
--- a/train_operator.py
+++ b/train_operator.py
@@ -78,10 +78,12 @@ def train_3d(args, config):
 def train_2d(args, config):
     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
     data_config = config['data']
+    print('loading data')
     dataset = DarcyFlow(data_config['datapath'],
                         nx=data_config['nx'], sub=data_config['sub'],
                         offset=data_config['offset'], num=data_config['n_sample'])
     train_loader = DataLoader(dataset, batch_size=config['train']['batchsize'], shuffle=True)
+    print('loaded data')
     model = FNN2d(modes1=config['model']['modes1'],
                   modes2=config['model']['modes2'],
                   fc_dim=config['model']['fc_dim'],
@@ -120,7 +122,8 @@ def train_2d(args, config):
                   activation=config['model']['activation'],
                   in_dim=4).to(device)
         optimizer = ACGD(max_params=Discriminator.parameters(), 
-                        min_params=Regressor.parameters())
+                        min_params=Regressor.parameters(), 
+                        tol=1e-4, lr_max=config['train']['lr_max'], lr_min=config['train']['lr_min'])
         # optimizer = BCGD(max_params=Discriminator.parameters(), 
         #                 min_params=Regressor.parameters(), 
         #                 lr_min=config['train']['lr_min'], 
diff --git a/train_utils/eval_2d.py b/train_utils/eval_2d.py
index d999dd9..46b9b0f 100644
--- a/train_utils/eval_2d.py
+++ b/train_utils/eval_2d.py
@@ -15,7 +15,17 @@ def eval_darcy(model,
                dataloader,
                config,
                device,
+               log=False,
+               entity='rishigundakaram', 
+               tags=None,
                use_tqdm=True):
+    if wandb and log: 
+        run = wandb.init(project=config['others']['project'],
+                         entity=entity,
+                         group=config['others']['group'],
+                         config=config,
+                         tags=tags, reinit=True,
+                         settings=wandb.Settings(start_method="fork"))
     model.eval()
     myloss = LpLoss(size_average=True)
     if use_tqdm:
@@ -56,7 +66,15 @@ def eval_darcy(model,
 
     print(f'==Averaged relative L2 error mean: {mean_err}, std error: {std_err}==\n'
           f'==Averaged equation error mean: {mean_f_err}, std error: {std_f_err}==')
-
+    if wandb and log:
+        wandb.log(
+            {
+                'mean f err': mean_f_err,
+                'std f err': std_f_err,
+                'mean L2 err': mean_err,
+                'std data err': std_err
+            }
+        )
 
 def eval_burgers(model,
                  dataloader,
diff --git a/train_utils/train_2d.py b/train_utils/train_2d.py
index 73684ea..331fcb7 100644
--- a/train_utils/train_2d.py
+++ b/train_utils/train_2d.py
@@ -146,10 +146,9 @@ def train_2d_operator_cgd(regressor,
 
     '''
     if rank == 0 and wandb and log:
-        print('here')
-        run = wandb.init(project=project,
+        run = wandb.init(project=config['others']['project'],
                          entity=entity,
-                         group=group,
+                         group=config['others']['group'],
                          config=config,
                          tags=tags, reinit=True,
                          settings=wandb.Settings(start_method="fork"))
