diff --git a/train_operator.py b/train_operator.py
index 53a79c8..ad70dac 100644
--- a/train_operator.py
+++ b/train_operator.py
@@ -3,12 +3,13 @@ from argparse import ArgumentParser
 import math
 import torch
 from torch.utils.data import DataLoader
+from CGDs import BCGD
 
 from solver.random_fields import GaussianRF
 from train_utils import Adam
 from train_utils.datasets import NSLoader, online_loader, DarcyFlow
 from train_utils.train_3d import mixed_train
-from train_utils.train_2d import train_2d_operator
+from train_utils.train_2d import train_2d_operator, train_2d_operator_cgd
 from models import FNN3d, FNN2d
 
 
@@ -77,10 +78,11 @@ def train_3d(args, config):
 def train_2d(args, config):
     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
     data_config = config['data']
-
+    print('loading data')
     dataset = DarcyFlow(data_config['datapath'],
                         nx=data_config['nx'], sub=data_config['sub'],
                         offset=data_config['offset'], num=data_config['n_sample'])
+    print('loaded data')
     train_loader = DataLoader(dataset, batch_size=config['train']['batchsize'], shuffle=True)
     model = FNN2d(modes1=config['model']['modes1'],
                   modes2=config['model']['modes2'],
@@ -94,17 +96,37 @@ def train_2d(args, config):
         model.load_state_dict(ckpt['model'])
         print('Weights loaded from %s' % ckpt_path)
 
-    optimizer = Adam(model.parameters(), betas=(0.9, 0.999),
-                         lr=config['train']['base_lr'])
-    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,
-                                                     milestones=config['train']['milestones'],
-                                                     gamma=config['train']['scheduler_gamma'])
-    train_2d_operator(model,
-                      train_loader,
-                      optimizer, scheduler,
-                      config, rank=0, log=args.log,
-                      project=config['others']['project'],
-                      group=config['others']['group'])
+    if config['model']['competitive'] == False: 
+        optimizer = Adam(model.parameters(), betas=(0.9, 0.999),
+                            lr=config['train']['base_lr'])
+        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,
+                                                        milestones=config['train']['milestones'],
+                                                        gamma=config['train']['scheduler_gamma'])
+        train_2d_operator(model,
+                        train_loader,
+                        optimizer, scheduler,
+                        config, rank=0, log=args.log,
+                        project=config['others']['project'],
+                        group=config['others']['group'],
+                        entity=config['others']['entity'])
+    else: 
+        Regressor = FNN2d(modes1=config['model']['modes1'],
+                  modes2=config['model']['modes2'],
+                  fc_dim=config['model']['fc_dim'],
+                  layers=config['model']['layers'],
+                  activation=config['model']['activation']).to(device)
+        Discriminator = FNN2d(modes1=config['model']['modes1'],
+                  modes2=config['model']['modes2'],
+                  fc_dim=config['model']['fc_dim'],
+                  layers=config['model']['layers'],
+                  activation=config['model']['activation']).to(device)
+        optimizer = BCGD(max_params=Discriminator.params(), 
+                        min_params=Regressor.params(), 
+                        lr_min=config['lr_min'], 
+                        lr_max=config['lr_max'],
+                        momentum=config['momentum'])
+        train_2d_operator_cgd(Regressor, Discriminator, optimizer, train_loader,
+                      config, rank=0)
 
 
 if __name__ == '__main__':
diff --git a/train_utils/losses.py b/train_utils/losses.py
index 393349e..55b4b0b 100644
--- a/train_utils/losses.py
+++ b/train_utils/losses.py
@@ -35,6 +35,20 @@ def FDM_Darcy(u, a, D=1):
     Du = - (auxx + auyy)
     return Du
 
+def weighted_darcy_loss(u, a, w): 
+    batchsize = u.size(0)
+    size = u.size(1)
+    u = u.reshape(batchsize, size, size)
+    a = a.reshape(batchsize, size, size)
+    lploss = LpLoss(size_average=True)
+
+    Du = FDM_Darcy(u, a)
+    f = torch.ones(Du.shape, device=u.device)
+    loss_f_uw = lploss.rel(Du, f)
+    loss_f_w = lploss.rel_weighted(Du, f, w)
+    
+    return loss_f_w, loss_f_uw
+    
 
 def darcy_loss(u, a):
     batchsize = u.size(0)
@@ -51,8 +65,7 @@ def darcy_loss(u, a):
     # boundary_u = u[:, index_x, index_y]
     # truth_u = torch.zeros(boundary_u.shape, device=u.device)
     # loss_u = lploss.abs(boundary_u, truth_u)
-
-    Du = FDM_Darcy(u, a)
+    Du = FDM_Darcy(u, a) 
     f = torch.ones(Du.shape, device=u.device)
     loss_f = lploss.rel(Du, f)
 
@@ -179,6 +192,22 @@ class LpLoss(object):
                 return torch.sum(all_norms)
 
         return all_norms
+    
+    def abs_weighted(self, x, y, w): 
+        num_examples = x.size()[0]
+
+        #Assume uniform mesh
+        h = 1.0 / (x.size()[1] - 1.0)
+        terms = w.view(-1)*(x.view(num_examples,-1) - y.view(num_examples,-1))
+        all_norms = (h**(self.d/self.p))*torch.norm(terms, self.p, 1)
+
+        if self.reduction:
+            if self.size_average:
+                return torch.mean(all_norms)
+            else:
+                return torch.sum(all_norms)
+
+        return all_norms
 
     def rel(self, x, y):
         num_examples = x.size()[0]
@@ -194,6 +223,20 @@ class LpLoss(object):
 
         return diff_norms/y_norms
 
+    def rel_weighted(self, x, y, w):
+        num_examples = x.size()[0]
+        terms = w.view(-1)*(x.view(num_examples,-1) - y.view(num_examples,-1))    
+        diff_norms = torch.norm(terms, self.p, 1)
+        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)
+
+        if self.reduction:
+            if self.size_average:
+                return torch.mean(diff_norms/y_norms)
+            else:
+                return torch.sum(diff_norms/y_norms)
+
+        return diff_norms/y_norms
+    
     def __call__(self, x, y):
         return self.rel(x, y)
 
diff --git a/train_utils/train_2d.py b/train_utils/train_2d.py
index 51a2b73..9f4079e 100644
--- a/train_utils/train_2d.py
+++ b/train_utils/train_2d.py
@@ -2,7 +2,7 @@ import numpy as np
 import torch
 from tqdm import tqdm
 from .utils import save_checkpoint
-from .losses import LpLoss, darcy_loss, PINO_loss
+from .losses import LpLoss, darcy_loss, PINO_loss, weighted_darcy_loss
 
 try:
     import wandb
@@ -14,10 +14,12 @@ def train_2d_operator(model,
                       train_loader,
                       optimizer, scheduler,
                       config,
-                      rank=0, log=False,
+                      rank=0, loss_fn = darcy_loss, 
+                      log=False,
                       project='PINO-2d-default',
                       group='default',
                       tags=['default'],
+                      entity='rishigundakaram',
                       use_tqdm=True,
                       profile=False):
     '''
@@ -41,7 +43,7 @@ def train_2d_operator(model,
     '''
     if rank == 0 and wandb and log:
         run = wandb.init(project=project,
-                         entity='hzzheng-pino',
+                         entity=entity,
                          group=group,
                          config=config,
                          tags=tags, reinit=True,
@@ -73,7 +75,7 @@ def train_2d_operator(model,
             data_loss = myloss(pred, y)
 
             a = x[..., 0]
-            f_loss = darcy_loss(pred, a)
+            f_loss = loss_fn(pred, a)
 
             loss = data_weight * data_loss + f_weight * f_loss
             loss.backward()
@@ -111,6 +113,116 @@ def train_2d_operator(model,
         run.finish()
     print('Done!')
 
+def train_2d_operator_cgd(regressor,
+                      discriminator, 
+                      optimizer,
+                      train_loader,
+                      config,
+                      rank=0, 
+                      log=False,
+                      project='CGD-PINO',
+                      group='Darcy',
+                      tags=['default'],
+                      use_tqdm=True,
+                      profile=False):
+    '''
+    train PINO on Darcy Flow
+    Args:
+        model:
+        train_loader:
+        optimizer:
+        scheduler:
+        config:
+        rank:
+        log:
+        project:
+        group:
+        tags:
+        use_tqdm:
+        profile:
+
+    Returns:
+
+    '''
+    if rank == 0 and wandb and log:
+        run = wandb.init(project=project,
+                         entity='rishigundakaram',
+                         group=group,
+                         config=config,
+                         tags=tags, reinit=True,
+                         settings=wandb.Settings(start_method="fork"))
+
+    data_weight = config['train']['xy_loss']
+    f_weight = config['train']['f_loss']
+    regressor.train()
+    discriminator.train()
+    myloss = LpLoss(size_average=True)
+    pbar = range(config['train']['epochs'])
+    if use_tqdm:
+        pbar = tqdm(pbar, dynamic_ncols=True, smoothing=0.1)
+    mesh = train_loader.dataset.mesh
+    mollifier = torch.sin(np.pi * mesh[..., 0]) * torch.sin(np.pi * mesh[..., 1]) * 0.001
+    mollifier = mollifier.to(rank)
+    for e in pbar:
+        loss_dict = {'train_loss': 0.0,
+                     'data_loss': 0.0,
+                     'f_loss_w': 0.0,
+                     'f_loss_uw': 0.0,
+                     'test_error': 0.0}
+        for x, y in train_loader:
+            x, y = x.to(rank), y.to(rank)
+
+            optimizer.zero_grad()
+
+            pred = regressor(x).reshape(y.shape)
+            pred = pred * mollifier
+
+            data_loss = myloss(pred, y)
+
+            a = x[..., 0]
+            w = discriminator(a)
+            f_loss_w, f_loss_uw = weighted_darcy_loss(pred, w, a)
+
+            loss = data_weight * data_loss + f_weight * f_loss_w
+            optimizer.step(loss)
+
+            loss_dict['train_loss'] += loss.item() * y.shape[0]
+            loss_dict['f_loss_w'] += f_loss_w.item() * y.shape[0]
+            loss_dict['f_loss_uw'] += f_loss_uw.item() * y.shape[0]
+            loss_dict['data_loss'] += data_loss.item() * y.shape[0]
+
+        train_loss_val = loss_dict['train_loss'] / len(train_loader.dataset)
+        f_loss_val_w = loss_dict['f_loss_w'] / len(train_loader.dataset)
+        f_loss_val_uw = loss_dict['f_loss_uw'] / len(train_loader.dataset)
+        data_loss_val = loss_dict['data_loss'] / len(train_loader.dataset)
+
+        if use_tqdm:
+            pbar.set_description(
+                (
+                    f'Epoch: {e}, train loss: {train_loss_val:.5f}, '
+                    f'f_loss_w: {f_loss_val_w:.5f}, '
+                    f'f_loss_uw: {f_loss_val_w:.5f}, '
+                    f'data loss: {data_loss_val:.5f}'
+                )
+            )
+        if wandb and log:
+            wandb.log(
+                {
+                    'train loss': train_loss_val,
+                    'f loss weighted': f_loss_val_w,
+                    'f loss unweighted': f_loss_val_uw,
+                    'data loss': data_loss_val
+                }
+            )
+    save_checkpoint(config['train']['save_dir'],
+                    config['train']['save_name'],
+                    regressor)
+    save_checkpoint(config['train']['save_dir'],
+                    config['train']['save_name'][:-3] + "-weights.pt",
+                    discriminator)
+    if wandb and log:
+        run.finish()
+    print('Done!')
 
 def train_2d_burger(model,
                     train_loader,
